传统数据库处理方式和大数据处理方式的区别
==============

为什么要使用大数据（Hadoop）解决方案，最关键的理由是我们生活在数据时代，每分每秒数据以极快的速度增长着，磁盘的存储容量也在快速增长，但与此同时磁盘数据读取速度（访问速度）却未能与时俱进。

#大数据处理方式简介

大数据指的是所涉及的数据量规模巨大到无法通过人工，在合理时间内达到截取、管理、处理、并整理成为人类所能解读的信息。大数据几乎无法使用大多数的数据库管理系统处理，而必须使用“在数十、数百甚至数千台服务器上同时平行运行的软件”。

大数据处理方案充分的利用了多线程，多任务的设计模式。目前大数据处理方式的理论是基于Google发布的三篇论文：分布式文件处理系统、MapReduce编程模型及分布式结构化数据存储系统。而具体的工业实现则是Apache Hadoop项目，它是实现了Google大数据处理理论的一款支持数据密集型分布式应用。

#传统数据库向大数据处理方式的转移

当我们从关系型数据库的存储方式向大数据处理方式转变的时候，需要注意一下几点：

* 数据迁移工作：和传统数据库不同大，数据方案的存储是基于分布式文件系统（DFS）构建的，目前可以使用Apache Sqoop来进行关系型数据库和Hadoop之间的海量数据传输，Sqoop可以将可以将标准的关系型数据库（例如 ： MySQL、Oracle、SQL Server等）中的数据导进到大数据（Hadoop）方案的HDFS中，它支持批量的导入导出。

* 访问接口改造：和传统数据库利用JDBC、JPA等标准的访问方式不同，大数据（Hadoop）方案是基于Map-Reduce API来进行访问的。Map-Reduce的编程需要一定的经验而且效率也不高，目前Apache提供了Pig Latin来改善此种状况，它在 MapReduce 的基础上创建了更简单的过程语言抽象，为 Hadoop 应用程序提供了一种更加接近结构化查询语言 (SQL) 的接口，当然其语法和SQL还是具有一些的差异，需要改造原有的数据接口代码。

* 半结构化/非结构化数据存储：传统数据库会利用CLOB或BLOB格式来放置半结构化及非结构化数据，但不同的关系型数据库对其处理方式也不尽相同。而且这种方式的缺点是显而易见，在数据库中检索CLOB/BLOB会导致比使用文件系统更大的开销，数据库 SAN 上的磁盘存储。而大数据Hadoop方案设计之初就是为了支持半结构化/非结构化数据的，对这部分数据的转移和访问需要做一定的改造。

* 数据统计分析方式改造：关系型数据库主要是通过商业智能软件（报表、OLAP、数据挖掘）等方式来对数据进行统计分析的，目前在大数据Hadoop方案上也提供了Hive作为数据仓库的方案，可以将Hadoop中的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。但目前Hive SQL和传统SQL还没有做到100%兼容（特别是目前还没有提供类似于Oracle PL-SQL等扩展了SQL语法的查询统计方式），一些复杂的统计分析工作还是需要通过Map-Reduce的编码工作来完成。


#大数据处理方式和传统数据库处理方式的比较

大数据（Hadoop）处理方式和传统的数据库（RDBMS）处理方式比较，主要有以下这些特点：

* 高性价比：和传统型数据需要存储在Oracle、EMC等专业的成本高昂的设备不同的是Hadoop是基于X86服务器集群进行搭建的，只需要不高的成本就可以搭建高吞吐量，高I/O的基础架构平台。而沿用传统的SAN方案，采用专用存储服务器来应对数据扩容的话，需要付出高额的前期投资；而且垂直化的专用存储管理软件，难以让不同的子系统之间共享存储资源，从而降低了投资回报率。

* 可扩展性：传统数据库通常采用纵向扩展的方式来实现，横向扩展能力最多不会超过20个节点，扩展能力有限，而且在横向扩展中需要操作系统、数据仓库等重新配置，需要较长的停机维护时间。大数据Hadoop方案使用的横向扩展，由于充分利用了多线程、多任务的方式进行，它可以在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中，基本不需要停机维护。

* 多样性：随着传感器、智能设备以及社交协作技术的激增，企业数据也变得更加复杂，因为它不仅包含传统的关系型数据，还包含来自网页、Web日志（包括单击流数据）、搜索索引、社交媒体论坛、电子邮件、文档等原始、半结构化和非结构化数据。传统的分析平台无法处理多种数据。传统型数据库一般只能存储结构化数据（作为非结构化数据存储的CLOB、BLOB方式，存在性能及其低下的问题），而大数据Hadoop方式原生的支持结构化、半结构化、非结构化海量数据的存储，并能保证其计算性能。

* 可用性：传统型数据库依赖于高可靠的硬件（存储需要做数据镜像）来保障其数据安全和可用性。大数据Hadoop方案则在设计初期就考虑了完善的自动容错机制，能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配，保障数据的容错安全和快速访问的可用性。

* 高性能：传统关系型数据库的数据处理极限是在100GB至1TB之间，随着数据量的增加，算法上的瓶颈会显现出来。而大数据（Map-Reduce）算法则是基于分布式系统而设计的，采用并行计算，能够处理TB甚至PB级别的数据，理论上只要处理集群能够无限制的扩展，其能够保证更大数据处理量的性能。

大数据（Hadoop）处理方式能够很好地解决传统数据库处理方式难以根除的容量和性能均衡分布的难题。

